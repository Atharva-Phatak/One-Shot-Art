{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One shot classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycc2ZOBMn_FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNrW2q8oq0l2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_folders():\n",
        "    ''' This Function returns the training and testing folders containing the respective classes'''\n",
        "\n",
        "    train_path =  '/content/drive/My Drive/Few Shot Painting Classification/Train'\n",
        "    test_path = '/content/drive/My Drive/Few Shot Painting Classification/Test'\n",
        "\n",
        "    train_folders = [os.path.join(train_path,label)  for label in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, label))]\n",
        "\n",
        "    test_folders = [os.path.join(test_path,label)  for label in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, label)) ]\n",
        "\n",
        "    random.seed(42)\n",
        "    random.shuffle(train_folders)\n",
        "    random.shuffle(test_folders)\n",
        "\n",
        "    return train_folders , test_folders\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVSjappar56b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Task_PaintingsData(object):\n",
        "    \n",
        "    ''' This function samples the training and testing files along with the corresponding labels.\n",
        "    Arguments:-\n",
        "    class_folders : The folders containing your training or testing classes.\n",
        "    num_classes : The number of classes you want to sample from all the classes i.e N-way\n",
        "    train_num : The number of total samples in your training set i.e sample set (K-shot)\n",
        "    test_num : The number of samples in test set ie. Query set'''\n",
        "\n",
        "    def __init__(self , class_folders , num_classes,train_num,test_num):\n",
        "\n",
        "        \n",
        "        self.class_folders = class_folders\n",
        "        self.num_classes = num_classes\n",
        "        self.train_num = train_num\n",
        "        self.test_num = test_num\n",
        "        #self.classes =  [x.split('/')[-1] for x in self.class_folders]\n",
        "\n",
        "        way_class_folders = random.sample(self.class_folders , self.num_classes)\n",
        "        labels = np.array(range(len(way_class_folders)))\n",
        "        labels = dict(zip(way_class_folders , labels))\n",
        "        samples = dict()\n",
        "\n",
        "        self.train_roots , self.test_roots = [] , []\n",
        "\n",
        "        for c in way_class_folders:\n",
        "\n",
        "            files = [os.path.join(c,x) for x in os.listdir(c)]\n",
        "            samples[c] = random.sample(files , len(files))\n",
        "            random.shuffle(samples[c])\n",
        "\n",
        "            self.train_roots += samples[c][:train_num]\n",
        "            self.test_roots += samples[c][train_num : train_num + test_num]\n",
        "        #print(labels)\n",
        "        self.train_labels = [labels[self.GetClass(x)] for x in self.train_roots]\n",
        "        self.test_labels  = [labels[self.GetClass(x)] for x in self.test_roots]\n",
        "\n",
        "    def GetClass(self, sample):\n",
        "\n",
        "        return os.path.join('/'.join(sample.split('/')[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U29iH6wGtjvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FewShotData(torch.utils.data.Dataset):\n",
        "\n",
        "    ''' This function is custom dataset class which will be required by dataloader.\n",
        "        The class returns the image and the corresponding label.\n",
        "        Argumnets:-\n",
        "        Task : It is the Task_PaintingsData object.\n",
        "        split : Whether to sample from test or train set.\n",
        "        transform : Transformations for the images\n",
        "        target_transform : Transformations for the labels'''\n",
        "\n",
        "    def __init__(self,task,split = 'train', transform = None, target_transform = None):\n",
        "\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.target_transform = target_transform\n",
        "        self.image_roots = self.task.train_roots if self.split == 'train' else self.task.test_roots\n",
        "        self.image_labels = self.task.train_labels if self.split == 'train' else self.task.test_labels\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.image_roots)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        image_root = self.image_roots[idx]\n",
        "        img = Image.open(image_root)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = self.image_labels[idx]\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "        \n",
        "        return img , label\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGWJpdhWA8HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FewShotBatchSampler(Sampler):\n",
        "\n",
        "    ''' This class provides a custom sampler which is required to construct the few shot episode.\n",
        "        Arguments:-\n",
        "        num_per_class : The number of samples per class.\n",
        "        num_instances : Total instances for that task.\n",
        "        num_class : The number of classes from which the examples are to be taken.\n",
        "        shuffle : Whether to shuffle the files or not.'''\n",
        "        \n",
        "    def __init__(self,num_per_class , num_instances , num_classes , shuffle = True):\n",
        "\n",
        "        self.num_per_class = num_per_class\n",
        "        self.num_instances = num_instances\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        if self.shuffle:\n",
        "\n",
        "            batch = [[i+j*self.num_instances for i in torch.randperm(self.num_instances)[:self.num_per_class]] for j in range(self.num_classes)]\n",
        "        \n",
        "        else:\n",
        "            batch = [[i+j*self.num_instances for i in range(self.num_instances)[:self.num_per_class]] for j in range(self.num_classes)]\n",
        "\n",
        "        batch = [item for sublist in batch for item in sublist]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batch)\n",
        "        return iter(batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OErpKnP3FfTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_few_shot_dataloaders(task,num_per_class = 1 , shuffle = False , split = 'train'):\n",
        "\n",
        "    '''This functions returns the dataloader which will be required for iterating throught the dataset'''\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((128,128) , interpolation = Image.NEAREST) ,\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.92206, 0.92206, 0.92206], std=[0.08426, 0.08426, 0.08426]) ,\n",
        "                                    ])\n",
        "    \n",
        "    data = FewShotData(task , split, transform)\n",
        "\n",
        "    if split == 'train':\n",
        "\n",
        "        sampler = FewShotBatchSampler(num_per_class,\n",
        "                                      num_classes = task.num_classes,\n",
        "                                      num_instances =task.train_num ,\n",
        "                                      shuffle = shuffle)\n",
        "    else:\n",
        "\n",
        "        sampler = FewShotBatchSampler(num_per_class,\n",
        "                                      num_classes = task.num_classes,\n",
        "                                      num_instances = task.test_num ,\n",
        "                                      shuffle = shuffle)\n",
        "    \n",
        "    loader = torch.utils.data.DataLoader(data , batch_size = num_per_class*task.num_classes, sampler = sampler)\n",
        "    return loader\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki-ZkUf24JD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the models\n",
        "\n",
        "class EmbeddingModule(nn.Module):\n",
        "    ''' This class implements the embedding module'''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(EmbeddingModule , self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3,64, kernel_size = 3 , padding = 0),\n",
        "                                    nn.BatchNorm2d(64 , momentum = 1 , affine = True),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64,64, kernel_size = 3, padding = 0),\n",
        "                                    nn.BatchNorm2d(64, momentum = 1, affine = True),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3, padding = 1),\n",
        "                                    nn.BatchNorm2d(64, momentum = 1, affine = True),\n",
        "                                    nn.ReLU())\n",
        "        \n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3, padding = 1),\n",
        "                                    nn.BatchNorm2d(64 , momentum = 1, affine = True),\n",
        "                                    nn.ReLU())\n",
        "    \n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3JWtUjTHHr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RelationNet(nn.Module):\n",
        "    ''' This class implements the relation module'''\n",
        "\n",
        "    def __init__(self , input_size , hidden_size):\n",
        "\n",
        "        super(RelationNet , self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(128,64,kernel_size = 3 , padding = 0),\n",
        "                                    nn.BatchNorm2d(64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(2))\n",
        "        \n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3 , padding = 0),\n",
        "                                    nn.BatchNorm2d(64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(2))\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size*6*6 , hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size , 1)\n",
        "    \n",
        "    def forward(self , x):\n",
        "\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.sigmoid(self.fc2(out))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t57v8QgOVKMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    '''Weights initializer for the networks'''\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())\n",
        "\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0*np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), scipy.stats.sem(a)\n",
        "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
        "    return m,h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhUD-mWpYNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_networks():\n",
        "\n",
        "    FEATURE_DIM = 64\n",
        "    RELATION_DIM = 8\n",
        "    CLASS_NUM = 5\n",
        "    SAMPLE_NUM_PER_CLASS = 1\n",
        "    BATCH_NUM_PER_CLASS = 15\n",
        "    EPISODE = 5000\n",
        "    TEST_EPISODE = 200\n",
        "    LEARNING_RATE = 0.001\n",
        "    \n",
        "    \n",
        "    \n",
        "    train_folders , test_folders = get_data_folders()\n",
        "    feature_enc = EmbeddingModule()\n",
        "    relation_model = RelationNet(FEATURE_DIM , RELATION_DIM)\n",
        "\n",
        "    feature_enc.apply(weights_init)\n",
        "    relation_model.apply(weights_init)\n",
        "\n",
        "    if torch.cuda.is_available:\n",
        "        #torch.cuda.empty_cache()\n",
        "        feature_enc.cuda()\n",
        "        relation_model.cuda()\n",
        "\n",
        "    feature_enc_optim = torch.optim.Adam(feature_enc.parameters() , lr = LEARNING_RATE)\n",
        "    feature_enc_scheduler = StepLR(feature_enc_optim , step_size = 200 , gamma = 0.5)\n",
        "    relation_model_optim = torch.optim.Adam(relation_model.parameters() , lr = LEARNING_RATE)\n",
        "    relation_model_scheduler = StepLR(relation_model_optim , step_size = 200 , gamma = 0.5)\n",
        "    print(\"Training...\")\n",
        "    last_acc = 0.0\n",
        "    train_loss = []\n",
        "    for ep in range(EPISODE):\n",
        "\n",
        "        feature_enc_scheduler.step(ep)\n",
        "        relation_model_scheduler.step(ep)\n",
        "\n",
        "        task = Task_PaintingsData(train_folders , CLASS_NUM, SAMPLE_NUM_PER_CLASS , BATCH_NUM_PER_CLASS)\n",
        "\n",
        "        sample_dataloader = get_few_shot_dataloaders(task , num_per_class = SAMPLE_NUM_PER_CLASS , shuffle = False, split = 'train')\n",
        "        batch_dataloader = get_few_shot_dataloaders(task , num_per_class = BATCH_NUM_PER_CLASS , shuffle = True , split = 'test')\n",
        "\n",
        "        samples , sample_labels = sample_dataloader.__iter__().next()\n",
        "        #print(samples.shape)\n",
        "\n",
        "        batches , batch_labels = batch_dataloader.__iter__().next()\n",
        "        #print(batches.shape)\n",
        "        \n",
        "        sample_features = feature_enc(Variable(samples).cuda())\n",
        "        #print(\"SAMPLE FEATURES\" ,sample_features.shape)\n",
        "        batch_features = feature_enc(Variable(batches).cuda())\n",
        "        #print('BATCH_FEATURES',batch_features.shape)\n",
        "        #print('Concatenating Features')\n",
        "        ext_sample_features = sample_features.unsqueeze(0).repeat(BATCH_NUM_PER_CLASS*CLASS_NUM,1,1,1,1)\n",
        "        #print('EXT_SAMPLE_FEATURES', ext_sample_features.shape)\n",
        "        ext_batch_features = batch_features.unsqueeze(0).repeat(SAMPLE_NUM_PER_CLASS*CLASS_NUM,1,1,1,1)\n",
        "        #print(\"Befor Trans\",ext_batch_features.shape)\n",
        "        ext_batch_features = torch.transpose(ext_batch_features,0,1)\n",
        "        #print(\"EXTENDED BATCH FEATURES\" ,ext_batch_features.shape)\n",
        "        #print(ext_batch_features[0])\n",
        "\n",
        "        \n",
        "\n",
        "        relation_pairs = torch.cat((ext_sample_features , ext_batch_features) , 2).view(-1,FEATURE_DIM*2,30,30)\n",
        "        #print(relation_pairs.shape)\n",
        "        #break\n",
        "        relations = relation_model(relation_pairs).view(-1,CLASS_NUM*SAMPLE_NUM_PER_CLASS)\n",
        "        #print(relations.shape)\n",
        "        \n",
        "        mse = nn.MSELoss()\n",
        "\n",
        "        one_hot_labels = Variable(torch.zeros(BATCH_NUM_PER_CLASS*CLASS_NUM, CLASS_NUM).scatter_(1, batch_labels.view(-1,1), 1)).cuda()\n",
        "        loss = mse(relations, one_hot_labels)\n",
        "\n",
        "        feature_enc.zero_grad()\n",
        "        relation_model.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(feature_enc.parameters(),0.5)\n",
        "        torch.nn.utils.clip_grad_norm_(relation_model.parameters(),0.5)\n",
        "\n",
        "        feature_enc_optim.step()\n",
        "        relation_model_optim.step()\n",
        "\n",
        "        if (ep + 1)%10 == 0:\n",
        "\n",
        "            print(f'EPISODE :{ep + 1} || LOSS : {loss.item()}')\n",
        "\n",
        "        if (ep+1)%50 == 0:\n",
        "\n",
        "            print('Testing')\n",
        "            accuracies = []\n",
        "            for i in range(TEST_EPISODE):\n",
        "\n",
        "                total_rewards = 0\n",
        "                counter = 0\n",
        "                task = Task_PaintingsData(test_folders,CLASS_NUM,1,10)\n",
        "                sample_dataloader = get_few_shot_dataloaders(task,num_per_class=1,split=\"train\",shuffle=False)\n",
        "\n",
        "                num_per_class = 3\n",
        "                test_dataloader = get_few_shot_dataloaders(task,num_per_class=num_per_class,split=\"test\",shuffle=True)\n",
        "                sample_images,sample_labels = sample_dataloader.__iter__().next()\n",
        "                for test_images,test_labels in test_dataloader:\n",
        "                    batch_size = test_labels.shape[0]\n",
        "                    # calculate features\n",
        "                    sample_features = feature_enc(Variable(sample_images).cuda()) # 5x64\n",
        "                    test_features = feature_enc(Variable(test_images).cuda()) # 20x64\n",
        "\n",
        "                    \n",
        "                    sample_features_ext = sample_features.unsqueeze(0).repeat(batch_size,1,1,1,1)\n",
        "                    test_features_ext = test_features.unsqueeze(0).repeat(1*CLASS_NUM,1,1,1,1)\n",
        "                    test_features_ext = torch.transpose(test_features_ext,0,1)\n",
        "                    relation_pairs = torch.cat((sample_features_ext,test_features_ext),2).view(-1,FEATURE_DIM*2,30,30)\n",
        "                    relations = relation_model(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "                    _,predict_labels = torch.max(relations.data,1)\n",
        "\n",
        "                    rewards = [1 if predict_labels[j]==test_labels[j] else 0 for j in range(batch_size)]\n",
        "\n",
        "                    total_rewards += np.sum(rewards)\n",
        "                    counter += batch_size\n",
        "                accuracy = total_rewards/1.0/counter\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "            test_accuracy,_= mean_confidence_interval(accuracies)\n",
        "\n",
        "            print(\"Test Accuracy:\",test_accuracy)\n",
        "\n",
        "            if test_accuracy > last_acc:\n",
        "\n",
        "                # save networks\n",
        "                torch.save(feature_enc.state_dict(),str(\"paintings_feature_encoder_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "                torch.save(relation_model.state_dict(),str(\"paintings_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "\n",
        "                print(\"save networks for episode:\",ep)\n",
        "\n",
        "                last_acc = test_accuracy\n",
        "        \n",
        "        \n",
        "\n",
        "    return train_loss,accuracies\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeTTK9-l0_By",
        "colab_type": "code",
        "outputId": "89065dd9-480c-4ef5-fcbf-32ba3a52a371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "loss,accuracy = train_networks()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE :10 || LOSS : 0.24950967729091644\n",
            "EPISODE :20 || LOSS : 0.16387146711349487\n",
            "EPISODE :30 || LOSS : 0.15532521903514862\n",
            "EPISODE :40 || LOSS : 0.15910230576992035\n",
            "EPISODE :50 || LOSS : 0.14458002150058746\n",
            "Testing\n",
            "Test Accuracy: 0.34166666666666673\n",
            "save networks for episode: 49\n",
            "EPISODE :60 || LOSS : 0.13951340317726135\n",
            "EPISODE :70 || LOSS : 0.1550045609474182\n",
            "EPISODE :80 || LOSS : 0.15388785302639008\n",
            "EPISODE :90 || LOSS : 0.13562358915805817\n",
            "EPISODE :100 || LOSS : 0.1595572978258133\n",
            "Testing\n",
            "Test Accuracy: 0.336\n",
            "EPISODE :110 || LOSS : 0.16376039385795593\n",
            "EPISODE :120 || LOSS : 0.15094415843486786\n",
            "EPISODE :130 || LOSS : 0.1493663489818573\n",
            "EPISODE :140 || LOSS : 0.14814560115337372\n",
            "EPISODE :150 || LOSS : 0.14552101492881775\n",
            "Testing\n",
            "Test Accuracy: 0.3516666666666666\n",
            "save networks for episode: 149\n",
            "EPISODE :160 || LOSS : 0.14619728922843933\n",
            "EPISODE :170 || LOSS : 0.1598415970802307\n",
            "EPISODE :180 || LOSS : 0.1365499496459961\n",
            "EPISODE :190 || LOSS : 0.16595694422721863\n",
            "EPISODE :200 || LOSS : 0.1366281658411026\n",
            "Testing\n",
            "Test Accuracy: 0.37233333333333335\n",
            "save networks for episode: 199\n",
            "EPISODE :210 || LOSS : 0.1492863893508911\n",
            "EPISODE :220 || LOSS : 0.139853835105896\n",
            "EPISODE :230 || LOSS : 0.13405051827430725\n",
            "EPISODE :240 || LOSS : 0.1592380553483963\n",
            "EPISODE :250 || LOSS : 0.14801929891109467\n",
            "Testing\n",
            "Test Accuracy: 0.3863333333333333\n",
            "save networks for episode: 249\n",
            "EPISODE :260 || LOSS : 0.14930729568004608\n",
            "EPISODE :270 || LOSS : 0.13618877530097961\n",
            "EPISODE :280 || LOSS : 0.1438693255186081\n",
            "EPISODE :290 || LOSS : 0.15145576000213623\n",
            "EPISODE :300 || LOSS : 0.1345473676919937\n",
            "Testing\n",
            "Test Accuracy: 0.3446666666666667\n",
            "EPISODE :310 || LOSS : 0.13826961815357208\n",
            "EPISODE :320 || LOSS : 0.15553759038448334\n",
            "EPISODE :330 || LOSS : 0.14625877141952515\n",
            "EPISODE :340 || LOSS : 0.13198219239711761\n",
            "EPISODE :350 || LOSS : 0.1505887359380722\n",
            "Testing\n",
            "Test Accuracy: 0.35866666666666674\n",
            "EPISODE :360 || LOSS : 0.1681145876646042\n",
            "EPISODE :370 || LOSS : 0.1253339946269989\n",
            "EPISODE :380 || LOSS : 0.13803444802761078\n",
            "EPISODE :390 || LOSS : 0.1608079969882965\n",
            "EPISODE :400 || LOSS : 0.13768485188484192\n",
            "Testing\n",
            "Test Accuracy: 0.3943333333333333\n",
            "save networks for episode: 399\n",
            "EPISODE :410 || LOSS : 0.14710365235805511\n",
            "EPISODE :420 || LOSS : 0.15158990025520325\n",
            "EPISODE :430 || LOSS : 0.1418938785791397\n",
            "EPISODE :440 || LOSS : 0.16259121894836426\n",
            "EPISODE :450 || LOSS : 0.15052026510238647\n",
            "Testing\n",
            "Test Accuracy: 0.38\n",
            "EPISODE :460 || LOSS : 0.13865569233894348\n",
            "EPISODE :470 || LOSS : 0.15354983508586884\n",
            "EPISODE :480 || LOSS : 0.15090195834636688\n",
            "EPISODE :490 || LOSS : 0.14581021666526794\n",
            "EPISODE :500 || LOSS : 0.13877952098846436\n",
            "Testing\n",
            "Test Accuracy: 0.3856666666666667\n",
            "EPISODE :510 || LOSS : 0.1355545073747635\n",
            "EPISODE :520 || LOSS : 0.14182189106941223\n",
            "EPISODE :530 || LOSS : 0.13811658322811127\n",
            "EPISODE :540 || LOSS : 0.15948915481567383\n",
            "EPISODE :550 || LOSS : 0.14300917088985443\n",
            "Testing\n",
            "Test Accuracy: 0.368\n",
            "EPISODE :560 || LOSS : 0.15277139842510223\n",
            "EPISODE :570 || LOSS : 0.1533360332250595\n",
            "EPISODE :580 || LOSS : 0.13069695234298706\n",
            "EPISODE :590 || LOSS : 0.14912371337413788\n",
            "EPISODE :600 || LOSS : 0.11658872663974762\n",
            "Testing\n",
            "Test Accuracy: 0.379\n",
            "EPISODE :610 || LOSS : 0.14090313017368317\n",
            "EPISODE :620 || LOSS : 0.14957502484321594\n",
            "EPISODE :630 || LOSS : 0.12332598865032196\n",
            "EPISODE :640 || LOSS : 0.15670780837535858\n",
            "EPISODE :650 || LOSS : 0.13167014718055725\n",
            "Testing\n",
            "Test Accuracy: 0.3803333333333333\n",
            "EPISODE :660 || LOSS : 0.16726666688919067\n",
            "EPISODE :670 || LOSS : 0.13789647817611694\n",
            "EPISODE :680 || LOSS : 0.1665734499692917\n",
            "EPISODE :690 || LOSS : 0.13587519526481628\n",
            "EPISODE :700 || LOSS : 0.14072614908218384\n",
            "Testing\n",
            "Test Accuracy: 0.38433333333333336\n",
            "EPISODE :710 || LOSS : 0.1542859822511673\n",
            "EPISODE :720 || LOSS : 0.1735467165708542\n",
            "EPISODE :730 || LOSS : 0.17178139090538025\n",
            "EPISODE :740 || LOSS : 0.15279483795166016\n",
            "EPISODE :750 || LOSS : 0.14669671654701233\n",
            "Testing\n",
            "Test Accuracy: 0.382\n",
            "EPISODE :760 || LOSS : 0.1392025500535965\n",
            "EPISODE :770 || LOSS : 0.126530721783638\n",
            "EPISODE :780 || LOSS : 0.1261563003063202\n",
            "EPISODE :790 || LOSS : 0.1564939320087433\n",
            "EPISODE :800 || LOSS : 0.14012715220451355\n",
            "Testing\n",
            "Test Accuracy: 0.377\n",
            "EPISODE :810 || LOSS : 0.14308112859725952\n",
            "EPISODE :820 || LOSS : 0.13731496036052704\n",
            "EPISODE :830 || LOSS : 0.16059230268001556\n",
            "EPISODE :840 || LOSS : 0.15927860140800476\n",
            "EPISODE :850 || LOSS : 0.15695105493068695\n",
            "Testing\n",
            "Test Accuracy: 0.3786666666666666\n",
            "EPISODE :860 || LOSS : 0.14511333405971527\n",
            "EPISODE :870 || LOSS : 0.146470308303833\n",
            "EPISODE :880 || LOSS : 0.13297489285469055\n",
            "EPISODE :890 || LOSS : 0.11097747832536697\n",
            "EPISODE :900 || LOSS : 0.15080419182777405\n",
            "Testing\n",
            "Test Accuracy: 0.37\n",
            "EPISODE :910 || LOSS : 0.12973809242248535\n",
            "EPISODE :920 || LOSS : 0.15268352627754211\n",
            "EPISODE :930 || LOSS : 0.14942693710327148\n",
            "EPISODE :940 || LOSS : 0.14365151524543762\n",
            "EPISODE :950 || LOSS : 0.1298198103904724\n",
            "Testing\n",
            "Test Accuracy: 0.397\n",
            "save networks for episode: 949\n",
            "EPISODE :960 || LOSS : 0.14175616204738617\n",
            "EPISODE :970 || LOSS : 0.1299038529396057\n",
            "EPISODE :980 || LOSS : 0.1547967940568924\n",
            "EPISODE :990 || LOSS : 0.1496790051460266\n",
            "EPISODE :1000 || LOSS : 0.13355515897274017\n",
            "Testing\n",
            "Test Accuracy: 0.3813333333333333\n",
            "EPISODE :1010 || LOSS : 0.14690923690795898\n",
            "EPISODE :1020 || LOSS : 0.12238392978906631\n",
            "EPISODE :1030 || LOSS : 0.14637112617492676\n",
            "EPISODE :1040 || LOSS : 0.14125053584575653\n",
            "EPISODE :1050 || LOSS : 0.1249929890036583\n",
            "Testing\n",
            "Test Accuracy: 0.39799999999999996\n",
            "save networks for episode: 1049\n",
            "EPISODE :1060 || LOSS : 0.12258756160736084\n",
            "EPISODE :1070 || LOSS : 0.1275249868631363\n",
            "EPISODE :1080 || LOSS : 0.14691047370433807\n",
            "EPISODE :1090 || LOSS : 0.12761728465557098\n",
            "EPISODE :1100 || LOSS : 0.15760314464569092\n",
            "Testing\n",
            "Test Accuracy: 0.3973333333333333\n",
            "EPISODE :1110 || LOSS : 0.13772882521152496\n",
            "EPISODE :1120 || LOSS : 0.14461368322372437\n",
            "EPISODE :1130 || LOSS : 0.16220413148403168\n",
            "EPISODE :1140 || LOSS : 0.1277875006198883\n",
            "EPISODE :1150 || LOSS : 0.1357501745223999\n",
            "Testing\n",
            "Test Accuracy: 0.38266666666666665\n",
            "EPISODE :1160 || LOSS : 0.1399269998073578\n",
            "EPISODE :1170 || LOSS : 0.1283162236213684\n",
            "EPISODE :1180 || LOSS : 0.12321630865335464\n",
            "EPISODE :1190 || LOSS : 0.12399671226739883\n",
            "EPISODE :1200 || LOSS : 0.1386745423078537\n",
            "Testing\n",
            "Test Accuracy: 0.3763333333333334\n",
            "EPISODE :1210 || LOSS : 0.13739454746246338\n",
            "EPISODE :1220 || LOSS : 0.15642009675502777\n",
            "EPISODE :1230 || LOSS : 0.14581753313541412\n",
            "EPISODE :1240 || LOSS : 0.146571546792984\n",
            "EPISODE :1250 || LOSS : 0.16673503816127777\n",
            "Testing\n",
            "Test Accuracy: 0.37\n",
            "EPISODE :1260 || LOSS : 0.12588335573673248\n",
            "EPISODE :1270 || LOSS : 0.1323612630367279\n",
            "EPISODE :1280 || LOSS : 0.15061762928962708\n",
            "EPISODE :1290 || LOSS : 0.1084584966301918\n",
            "EPISODE :1300 || LOSS : 0.1559075117111206\n",
            "Testing\n",
            "Test Accuracy: 0.37433333333333335\n",
            "EPISODE :1310 || LOSS : 0.16428826749324799\n",
            "EPISODE :1320 || LOSS : 0.15718241035938263\n",
            "EPISODE :1330 || LOSS : 0.11542466282844543\n",
            "EPISODE :1340 || LOSS : 0.1435694843530655\n",
            "EPISODE :1350 || LOSS : 0.14218497276306152\n",
            "Testing\n",
            "Test Accuracy: 0.397\n",
            "EPISODE :1360 || LOSS : 0.1376224160194397\n",
            "EPISODE :1370 || LOSS : 0.14999127388000488\n",
            "EPISODE :1380 || LOSS : 0.17269612848758698\n",
            "EPISODE :1390 || LOSS : 0.11661744862794876\n",
            "EPISODE :1400 || LOSS : 0.14933457970619202\n",
            "Testing\n",
            "Test Accuracy: 0.38299999999999995\n",
            "EPISODE :1410 || LOSS : 0.13529516756534576\n",
            "EPISODE :1420 || LOSS : 0.11881180852651596\n",
            "EPISODE :1430 || LOSS : 0.1534094363451004\n",
            "EPISODE :1440 || LOSS : 0.13597261905670166\n",
            "EPISODE :1450 || LOSS : 0.1318569779396057\n",
            "Testing\n",
            "Test Accuracy: 0.3853333333333333\n",
            "EPISODE :1460 || LOSS : 0.11735603958368301\n",
            "EPISODE :1470 || LOSS : 0.13104113936424255\n",
            "EPISODE :1480 || LOSS : 0.10885545611381531\n",
            "EPISODE :1490 || LOSS : 0.15352560579776764\n",
            "EPISODE :1500 || LOSS : 0.13983777165412903\n",
            "Testing\n",
            "Test Accuracy: 0.39466666666666667\n",
            "EPISODE :1510 || LOSS : 0.12894611060619354\n",
            "EPISODE :1520 || LOSS : 0.15352144837379456\n",
            "EPISODE :1530 || LOSS : 0.1537138819694519\n",
            "EPISODE :1540 || LOSS : 0.15698149800300598\n",
            "EPISODE :1550 || LOSS : 0.15263207256793976\n",
            "Testing\n",
            "Test Accuracy: 0.381\n",
            "EPISODE :1560 || LOSS : 0.12230142951011658\n",
            "EPISODE :1570 || LOSS : 0.16469328105449677\n",
            "EPISODE :1580 || LOSS : 0.12773968279361725\n",
            "EPISODE :1590 || LOSS : 0.1559964418411255\n",
            "EPISODE :1600 || LOSS : 0.16418449580669403\n",
            "Testing\n",
            "Test Accuracy: 0.39166666666666666\n",
            "EPISODE :1610 || LOSS : 0.13303613662719727\n",
            "EPISODE :1620 || LOSS : 0.15034928917884827\n",
            "EPISODE :1630 || LOSS : 0.13069510459899902\n",
            "EPISODE :1640 || LOSS : 0.15193873643875122\n",
            "EPISODE :1650 || LOSS : 0.1248152032494545\n",
            "Testing\n",
            "Test Accuracy: 0.4023333333333333\n",
            "save networks for episode: 1649\n",
            "EPISODE :1660 || LOSS : 0.13372953236103058\n",
            "EPISODE :1670 || LOSS : 0.12142651528120041\n",
            "EPISODE :1680 || LOSS : 0.1280253827571869\n",
            "EPISODE :1690 || LOSS : 0.14422635734081268\n",
            "EPISODE :1700 || LOSS : 0.1208261176943779\n",
            "Testing\n",
            "Test Accuracy: 0.391\n",
            "EPISODE :1710 || LOSS : 0.1357044279575348\n",
            "EPISODE :1720 || LOSS : 0.13679639995098114\n",
            "EPISODE :1730 || LOSS : 0.12088742852210999\n",
            "EPISODE :1740 || LOSS : 0.1598900705575943\n",
            "EPISODE :1750 || LOSS : 0.1588950902223587\n",
            "Testing\n",
            "Test Accuracy: 0.40399999999999997\n",
            "save networks for episode: 1749\n",
            "EPISODE :1760 || LOSS : 0.12050209194421768\n",
            "EPISODE :1770 || LOSS : 0.1408243179321289\n",
            "EPISODE :1780 || LOSS : 0.10417892038822174\n",
            "EPISODE :1790 || LOSS : 0.16205409169197083\n",
            "EPISODE :1800 || LOSS : 0.11865830421447754\n",
            "Testing\n",
            "Test Accuracy: 0.37799999999999995\n",
            "EPISODE :1810 || LOSS : 0.13669098913669586\n",
            "EPISODE :1820 || LOSS : 0.13771702349185944\n",
            "EPISODE :1830 || LOSS : 0.12869155406951904\n",
            "EPISODE :1840 || LOSS : 0.12653307616710663\n",
            "EPISODE :1850 || LOSS : 0.1275123506784439\n",
            "Testing\n",
            "Test Accuracy: 0.39366666666666666\n",
            "EPISODE :1860 || LOSS : 0.1511528193950653\n",
            "EPISODE :1870 || LOSS : 0.11669878661632538\n",
            "EPISODE :1880 || LOSS : 0.15660829842090607\n",
            "EPISODE :1890 || LOSS : 0.13741828501224518\n",
            "EPISODE :1900 || LOSS : 0.14349891245365143\n",
            "Testing\n",
            "Test Accuracy: 0.386\n",
            "EPISODE :1910 || LOSS : 0.1579420268535614\n",
            "EPISODE :1920 || LOSS : 0.13978241384029388\n",
            "EPISODE :1930 || LOSS : 0.11700139194726944\n",
            "EPISODE :1940 || LOSS : 0.12781071662902832\n",
            "EPISODE :1950 || LOSS : 0.1489863097667694\n",
            "Testing\n",
            "Test Accuracy: 0.38833333333333336\n",
            "EPISODE :1960 || LOSS : 0.12568703293800354\n",
            "EPISODE :1970 || LOSS : 0.10797971487045288\n",
            "EPISODE :1980 || LOSS : 0.13077113032341003\n",
            "EPISODE :1990 || LOSS : 0.09582926332950592\n",
            "EPISODE :2000 || LOSS : 0.135697141289711\n",
            "Testing\n",
            "Test Accuracy: 0.37866666666666665\n",
            "EPISODE :2010 || LOSS : 0.14214365184307098\n",
            "EPISODE :2020 || LOSS : 0.14268295466899872\n",
            "EPISODE :2030 || LOSS : 0.11822526901960373\n",
            "EPISODE :2040 || LOSS : 0.1439104676246643\n",
            "EPISODE :2050 || LOSS : 0.14260292053222656\n",
            "Testing\n",
            "Test Accuracy: 0.38799999999999996\n",
            "EPISODE :2060 || LOSS : 0.1334521770477295\n",
            "EPISODE :2070 || LOSS : 0.12227467447519302\n",
            "EPISODE :2080 || LOSS : 0.13899454474449158\n",
            "EPISODE :2090 || LOSS : 0.14184758067131042\n",
            "EPISODE :2100 || LOSS : 0.12615205347537994\n",
            "Testing\n",
            "Test Accuracy: 0.3816666666666667\n",
            "EPISODE :2110 || LOSS : 0.14445723593235016\n",
            "EPISODE :2120 || LOSS : 0.1304602175951004\n",
            "EPISODE :2130 || LOSS : 0.1304243803024292\n",
            "EPISODE :2140 || LOSS : 0.1430235058069229\n",
            "EPISODE :2150 || LOSS : 0.12842097878456116\n",
            "Testing\n",
            "Test Accuracy: 0.39099999999999996\n",
            "EPISODE :2160 || LOSS : 0.14067962765693665\n",
            "EPISODE :2170 || LOSS : 0.10434781759977341\n",
            "EPISODE :2180 || LOSS : 0.1402859389781952\n",
            "EPISODE :2190 || LOSS : 0.14181971549987793\n",
            "EPISODE :2200 || LOSS : 0.13287892937660217\n",
            "Testing\n",
            "Test Accuracy: 0.37800000000000006\n",
            "EPISODE :2210 || LOSS : 0.1404692381620407\n",
            "EPISODE :2220 || LOSS : 0.11036808043718338\n",
            "EPISODE :2230 || LOSS : 0.14123167097568512\n",
            "EPISODE :2240 || LOSS : 0.12111375480890274\n",
            "EPISODE :2250 || LOSS : 0.1507110446691513\n",
            "Testing\n",
            "Test Accuracy: 0.41900000000000004\n",
            "save networks for episode: 2249\n",
            "EPISODE :2260 || LOSS : 0.11602841317653656\n",
            "EPISODE :2270 || LOSS : 0.11421128362417221\n",
            "EPISODE :2280 || LOSS : 0.1315499097108841\n",
            "EPISODE :2290 || LOSS : 0.12299221754074097\n",
            "EPISODE :2300 || LOSS : 0.11581644415855408\n",
            "Testing\n",
            "Test Accuracy: 0.37\n",
            "EPISODE :2310 || LOSS : 0.14618097245693207\n",
            "EPISODE :2320 || LOSS : 0.10038140416145325\n",
            "EPISODE :2330 || LOSS : 0.1353633850812912\n",
            "EPISODE :2340 || LOSS : 0.11964061111211777\n",
            "EPISODE :2350 || LOSS : 0.11767016351222992\n",
            "Testing\n",
            "Test Accuracy: 0.3753333333333334\n",
            "EPISODE :2360 || LOSS : 0.15020139515399933\n",
            "EPISODE :2370 || LOSS : 0.14840251207351685\n",
            "EPISODE :2380 || LOSS : 0.14880718290805817\n",
            "EPISODE :2390 || LOSS : 0.1421562284231186\n",
            "EPISODE :2400 || LOSS : 0.14406070113182068\n",
            "Testing\n",
            "Test Accuracy: 0.393\n",
            "EPISODE :2410 || LOSS : 0.12785600125789642\n",
            "EPISODE :2420 || LOSS : 0.15076886117458344\n",
            "EPISODE :2430 || LOSS : 0.13574986159801483\n",
            "EPISODE :2440 || LOSS : 0.14806579053401947\n",
            "EPISODE :2450 || LOSS : 0.12665846943855286\n",
            "Testing\n",
            "Test Accuracy: 0.389\n",
            "EPISODE :2460 || LOSS : 0.1402856707572937\n",
            "EPISODE :2470 || LOSS : 0.12647680938243866\n",
            "EPISODE :2480 || LOSS : 0.11657211184501648\n",
            "EPISODE :2490 || LOSS : 0.13274063169956207\n",
            "EPISODE :2500 || LOSS : 0.13193124532699585\n",
            "Testing\n",
            "Test Accuracy: 0.38733333333333336\n",
            "EPISODE :2510 || LOSS : 0.11343306303024292\n",
            "EPISODE :2520 || LOSS : 0.1198873296380043\n",
            "EPISODE :2530 || LOSS : 0.13628506660461426\n",
            "EPISODE :2540 || LOSS : 0.13826730847358704\n",
            "EPISODE :2550 || LOSS : 0.15740013122558594\n",
            "Testing\n",
            "Test Accuracy: 0.392\n",
            "EPISODE :2560 || LOSS : 0.1393088400363922\n",
            "EPISODE :2570 || LOSS : 0.16042707860469818\n",
            "EPISODE :2580 || LOSS : 0.1271192878484726\n",
            "EPISODE :2590 || LOSS : 0.15567909181118011\n",
            "EPISODE :2600 || LOSS : 0.11422741413116455\n",
            "Testing\n",
            "Test Accuracy: 0.38933333333333336\n",
            "EPISODE :2610 || LOSS : 0.1403728425502777\n",
            "EPISODE :2620 || LOSS : 0.12705709040164948\n",
            "EPISODE :2630 || LOSS : 0.12993404269218445\n",
            "EPISODE :2640 || LOSS : 0.12781044840812683\n",
            "EPISODE :2650 || LOSS : 0.12378879636526108\n",
            "Testing\n",
            "Test Accuracy: 0.3813333333333333\n",
            "EPISODE :2660 || LOSS : 0.14020681381225586\n",
            "EPISODE :2670 || LOSS : 0.13657216727733612\n",
            "EPISODE :2680 || LOSS : 0.15380160510540009\n",
            "EPISODE :2690 || LOSS : 0.15805867314338684\n",
            "EPISODE :2700 || LOSS : 0.14084281027317047\n",
            "Testing\n",
            "Test Accuracy: 0.38066666666666665\n",
            "EPISODE :2710 || LOSS : 0.12413526326417923\n",
            "EPISODE :2720 || LOSS : 0.1385524868965149\n",
            "EPISODE :2730 || LOSS : 0.12757925689220428\n",
            "EPISODE :2740 || LOSS : 0.12844040989875793\n",
            "EPISODE :2750 || LOSS : 0.13029201328754425\n",
            "Testing\n",
            "Test Accuracy: 0.37199999999999994\n",
            "EPISODE :2760 || LOSS : 0.1347830891609192\n",
            "EPISODE :2770 || LOSS : 0.1536194384098053\n",
            "EPISODE :2780 || LOSS : 0.12559938430786133\n",
            "EPISODE :2790 || LOSS : 0.13389351963996887\n",
            "EPISODE :2800 || LOSS : 0.15913322567939758\n",
            "Testing\n",
            "Test Accuracy: 0.38733333333333336\n",
            "EPISODE :2810 || LOSS : 0.11846352368593216\n",
            "EPISODE :2820 || LOSS : 0.1267501711845398\n",
            "EPISODE :2830 || LOSS : 0.12815815210342407\n",
            "EPISODE :2840 || LOSS : 0.1345173865556717\n",
            "EPISODE :2850 || LOSS : 0.15718114376068115\n",
            "Testing\n",
            "Test Accuracy: 0.39466666666666667\n",
            "EPISODE :2860 || LOSS : 0.14471381902694702\n",
            "EPISODE :2870 || LOSS : 0.1555212140083313\n",
            "EPISODE :2880 || LOSS : 0.13823862373828888\n",
            "EPISODE :2890 || LOSS : 0.15696914494037628\n",
            "EPISODE :2900 || LOSS : 0.15804865956306458\n",
            "Testing\n",
            "Test Accuracy: 0.3746666666666667\n",
            "EPISODE :2910 || LOSS : 0.1244945377111435\n",
            "EPISODE :2920 || LOSS : 0.14118346571922302\n",
            "EPISODE :2930 || LOSS : 0.14025995135307312\n",
            "EPISODE :2940 || LOSS : 0.15693828463554382\n",
            "EPISODE :2950 || LOSS : 0.14370395243167877\n",
            "Testing\n",
            "Test Accuracy: 0.38233333333333336\n",
            "EPISODE :2960 || LOSS : 0.12138374894857407\n",
            "EPISODE :2970 || LOSS : 0.14840608835220337\n",
            "EPISODE :2980 || LOSS : 0.10458111763000488\n",
            "EPISODE :2990 || LOSS : 0.14299839735031128\n",
            "EPISODE :3000 || LOSS : 0.12800700962543488\n",
            "Testing\n",
            "Test Accuracy: 0.3856666666666667\n",
            "EPISODE :3010 || LOSS : 0.14086267352104187\n",
            "EPISODE :3020 || LOSS : 0.15276925265789032\n",
            "EPISODE :3030 || LOSS : 0.11265809088945389\n",
            "EPISODE :3040 || LOSS : 0.14057670533657074\n",
            "EPISODE :3050 || LOSS : 0.13852734863758087\n",
            "Testing\n",
            "Test Accuracy: 0.3916666666666667\n",
            "EPISODE :3060 || LOSS : 0.151066392660141\n",
            "EPISODE :3070 || LOSS : 0.15617720782756805\n",
            "EPISODE :3080 || LOSS : 0.1277192384004593\n",
            "EPISODE :3090 || LOSS : 0.15531428158283234\n",
            "EPISODE :3100 || LOSS : 0.11059848964214325\n",
            "Testing\n",
            "Test Accuracy: 0.4036666666666667\n",
            "EPISODE :3110 || LOSS : 0.1325952559709549\n",
            "EPISODE :3120 || LOSS : 0.14288541674613953\n",
            "EPISODE :3130 || LOSS : 0.1424189656972885\n",
            "EPISODE :3140 || LOSS : 0.12549102306365967\n",
            "EPISODE :3150 || LOSS : 0.14440366625785828\n",
            "Testing\n",
            "Test Accuracy: 0.3903333333333333\n",
            "EPISODE :3160 || LOSS : 0.16064466536045074\n",
            "EPISODE :3170 || LOSS : 0.12721754610538483\n",
            "EPISODE :3180 || LOSS : 0.11369287222623825\n",
            "EPISODE :3190 || LOSS : 0.13123294711112976\n",
            "EPISODE :3200 || LOSS : 0.13461990654468536\n",
            "Testing\n",
            "Test Accuracy: 0.38966666666666666\n",
            "EPISODE :3210 || LOSS : 0.1342538446187973\n",
            "EPISODE :3220 || LOSS : 0.11251265555620193\n",
            "EPISODE :3230 || LOSS : 0.1258210837841034\n",
            "EPISODE :3240 || LOSS : 0.14506776630878448\n",
            "EPISODE :3250 || LOSS : 0.12966366112232208\n",
            "Testing\n",
            "Test Accuracy: 0.3833333333333333\n",
            "EPISODE :3260 || LOSS : 0.12228995561599731\n",
            "EPISODE :3270 || LOSS : 0.15855328738689423\n",
            "EPISODE :3280 || LOSS : 0.13888441026210785\n",
            "EPISODE :3290 || LOSS : 0.1377667486667633\n",
            "EPISODE :3300 || LOSS : 0.15271225571632385\n",
            "Testing\n",
            "Test Accuracy: 0.3706666666666667\n",
            "EPISODE :3310 || LOSS : 0.16354046761989594\n",
            "EPISODE :3320 || LOSS : 0.1173454001545906\n",
            "EPISODE :3330 || LOSS : 0.12459418922662735\n",
            "EPISODE :3340 || LOSS : 0.15567882359027863\n",
            "EPISODE :3350 || LOSS : 0.15306538343429565\n",
            "Testing\n",
            "Test Accuracy: 0.3746666666666667\n",
            "EPISODE :3360 || LOSS : 0.125086709856987\n",
            "EPISODE :3370 || LOSS : 0.1315767616033554\n",
            "EPISODE :3380 || LOSS : 0.1047484427690506\n",
            "EPISODE :3390 || LOSS : 0.13093437254428864\n",
            "EPISODE :3400 || LOSS : 0.14275698363780975\n",
            "Testing\n",
            "Test Accuracy: 0.37933333333333336\n",
            "EPISODE :3410 || LOSS : 0.1353919506072998\n",
            "EPISODE :3420 || LOSS : 0.12183289229869843\n",
            "EPISODE :3430 || LOSS : 0.13100466132164001\n",
            "EPISODE :3440 || LOSS : 0.12360196560621262\n",
            "EPISODE :3450 || LOSS : 0.14354604482650757\n",
            "Testing\n",
            "Test Accuracy: 0.397\n",
            "EPISODE :3460 || LOSS : 0.1701575368642807\n",
            "EPISODE :3470 || LOSS : 0.12849853932857513\n",
            "EPISODE :3480 || LOSS : 0.13121749460697174\n",
            "EPISODE :3490 || LOSS : 0.13000977039337158\n",
            "EPISODE :3500 || LOSS : 0.1272110939025879\n",
            "Testing\n",
            "Test Accuracy: 0.38766666666666666\n",
            "EPISODE :3510 || LOSS : 0.10386885702610016\n",
            "EPISODE :3520 || LOSS : 0.1604834794998169\n",
            "EPISODE :3530 || LOSS : 0.14868484437465668\n",
            "EPISODE :3540 || LOSS : 0.14804822206497192\n",
            "EPISODE :3550 || LOSS : 0.1335751861333847\n",
            "Testing\n",
            "Test Accuracy: 0.39299999999999996\n",
            "EPISODE :3560 || LOSS : 0.11346931755542755\n",
            "EPISODE :3570 || LOSS : 0.12563592195510864\n",
            "EPISODE :3580 || LOSS : 0.1381627470254898\n",
            "EPISODE :3590 || LOSS : 0.14050492644309998\n",
            "EPISODE :3600 || LOSS : 0.1048496887087822\n",
            "Testing\n",
            "Test Accuracy: 0.38733333333333336\n",
            "EPISODE :3610 || LOSS : 0.11326416581869125\n",
            "EPISODE :3620 || LOSS : 0.1352315992116928\n",
            "EPISODE :3630 || LOSS : 0.1321611851453781\n",
            "EPISODE :3640 || LOSS : 0.16254599392414093\n",
            "EPISODE :3650 || LOSS : 0.12512056529521942\n",
            "Testing\n",
            "Test Accuracy: 0.3766666666666667\n",
            "EPISODE :3660 || LOSS : 0.12654857337474823\n",
            "EPISODE :3670 || LOSS : 0.12485715746879578\n",
            "EPISODE :3680 || LOSS : 0.12632694840431213\n",
            "EPISODE :3690 || LOSS : 0.15044741332530975\n",
            "EPISODE :3700 || LOSS : 0.1660374402999878\n",
            "Testing\n",
            "Test Accuracy: 0.38833333333333336\n",
            "EPISODE :3710 || LOSS : 0.12971000373363495\n",
            "EPISODE :3720 || LOSS : 0.148651584982872\n",
            "EPISODE :3730 || LOSS : 0.13907544314861298\n",
            "EPISODE :3740 || LOSS : 0.12073206156492233\n",
            "EPISODE :3750 || LOSS : 0.15674543380737305\n",
            "Testing\n",
            "Test Accuracy: 0.38799999999999996\n",
            "EPISODE :3760 || LOSS : 0.16516581177711487\n",
            "EPISODE :3770 || LOSS : 0.1363855004310608\n",
            "EPISODE :3780 || LOSS : 0.13807521760463715\n",
            "EPISODE :3790 || LOSS : 0.13027602434158325\n",
            "EPISODE :3800 || LOSS : 0.13628052175045013\n",
            "Testing\n",
            "Test Accuracy: 0.387\n",
            "EPISODE :3810 || LOSS : 0.12673842906951904\n",
            "EPISODE :3820 || LOSS : 0.15139582753181458\n",
            "EPISODE :3830 || LOSS : 0.12785038352012634\n",
            "EPISODE :3840 || LOSS : 0.14927327632904053\n",
            "EPISODE :3850 || LOSS : 0.12437684088945389\n",
            "Testing\n",
            "Test Accuracy: 0.36566666666666664\n",
            "EPISODE :3860 || LOSS : 0.1290116012096405\n",
            "EPISODE :3870 || LOSS : 0.1373613327741623\n",
            "EPISODE :3880 || LOSS : 0.1442943960428238\n",
            "EPISODE :3890 || LOSS : 0.15356524288654327\n",
            "EPISODE :3900 || LOSS : 0.14582155644893646\n",
            "Testing\n",
            "Test Accuracy: 0.3776666666666667\n",
            "EPISODE :3910 || LOSS : 0.15969744324684143\n",
            "EPISODE :3920 || LOSS : 0.14593617618083954\n",
            "EPISODE :3930 || LOSS : 0.15516619384288788\n",
            "EPISODE :3940 || LOSS : 0.12944433093070984\n",
            "EPISODE :3950 || LOSS : 0.12853184342384338\n",
            "Testing\n",
            "Test Accuracy: 0.396\n",
            "EPISODE :3960 || LOSS : 0.1365998089313507\n",
            "EPISODE :3970 || LOSS : 0.11856628954410553\n",
            "EPISODE :3980 || LOSS : 0.10464309900999069\n",
            "EPISODE :3990 || LOSS : 0.11968420445919037\n",
            "EPISODE :4000 || LOSS : 0.1146591305732727\n",
            "Testing\n",
            "Test Accuracy: 0.36733333333333335\n",
            "EPISODE :4010 || LOSS : 0.14607983827590942\n",
            "EPISODE :4020 || LOSS : 0.13694043457508087\n",
            "EPISODE :4030 || LOSS : 0.1336481273174286\n",
            "EPISODE :4040 || LOSS : 0.12917956709861755\n",
            "EPISODE :4050 || LOSS : 0.14590127766132355\n",
            "Testing\n",
            "Test Accuracy: 0.3973333333333333\n",
            "EPISODE :4060 || LOSS : 0.1455586701631546\n",
            "EPISODE :4070 || LOSS : 0.16882315278053284\n",
            "EPISODE :4080 || LOSS : 0.13469348847866058\n",
            "EPISODE :4090 || LOSS : 0.14688517153263092\n",
            "EPISODE :4100 || LOSS : 0.12843233346939087\n",
            "Testing\n",
            "Test Accuracy: 0.3863333333333333\n",
            "EPISODE :4110 || LOSS : 0.12808655202388763\n",
            "EPISODE :4120 || LOSS : 0.13585831224918365\n",
            "EPISODE :4130 || LOSS : 0.09685862064361572\n",
            "EPISODE :4140 || LOSS : 0.1568542867898941\n",
            "EPISODE :4150 || LOSS : 0.13829933106899261\n",
            "Testing\n",
            "Test Accuracy: 0.366\n",
            "EPISODE :4160 || LOSS : 0.12905561923980713\n",
            "EPISODE :4170 || LOSS : 0.13496807217597961\n",
            "EPISODE :4180 || LOSS : 0.1488053798675537\n",
            "EPISODE :4190 || LOSS : 0.11766428500413895\n",
            "EPISODE :4200 || LOSS : 0.14800883829593658\n",
            "Testing\n",
            "Test Accuracy: 0.37400000000000005\n",
            "EPISODE :4210 || LOSS : 0.11626404523849487\n",
            "EPISODE :4220 || LOSS : 0.12949834764003754\n",
            "EPISODE :4230 || LOSS : 0.11829081177711487\n",
            "EPISODE :4240 || LOSS : 0.12867487967014313\n",
            "EPISODE :4250 || LOSS : 0.15698349475860596\n",
            "Testing\n",
            "Test Accuracy: 0.38833333333333336\n",
            "EPISODE :4260 || LOSS : 0.1347293108701706\n",
            "EPISODE :4270 || LOSS : 0.11855188012123108\n",
            "EPISODE :4280 || LOSS : 0.13157038390636444\n",
            "EPISODE :4290 || LOSS : 0.10910108685493469\n",
            "EPISODE :4300 || LOSS : 0.13277453184127808\n",
            "Testing\n",
            "Test Accuracy: 0.3766666666666667\n",
            "EPISODE :4310 || LOSS : 0.1372215300798416\n",
            "EPISODE :4320 || LOSS : 0.1165778785943985\n",
            "EPISODE :4330 || LOSS : 0.13419310748577118\n",
            "EPISODE :4340 || LOSS : 0.1405470073223114\n",
            "EPISODE :4350 || LOSS : 0.1014803871512413\n",
            "Testing\n",
            "Test Accuracy: 0.38933333333333336\n",
            "EPISODE :4360 || LOSS : 0.13914494216442108\n",
            "EPISODE :4370 || LOSS : 0.14981970191001892\n",
            "EPISODE :4380 || LOSS : 0.1382816582918167\n",
            "EPISODE :4390 || LOSS : 0.15250104665756226\n",
            "EPISODE :4400 || LOSS : 0.13898417353630066\n",
            "Testing\n",
            "Test Accuracy: 0.38299999999999995\n",
            "EPISODE :4410 || LOSS : 0.1582392156124115\n",
            "EPISODE :4420 || LOSS : 0.13758976757526398\n",
            "EPISODE :4430 || LOSS : 0.12051220238208771\n",
            "EPISODE :4440 || LOSS : 0.14026659727096558\n",
            "EPISODE :4450 || LOSS : 0.12691278755664825\n",
            "Testing\n",
            "Test Accuracy: 0.3853333333333333\n",
            "EPISODE :4460 || LOSS : 0.15888498723506927\n",
            "EPISODE :4470 || LOSS : 0.14573140442371368\n",
            "EPISODE :4480 || LOSS : 0.15110941231250763\n",
            "EPISODE :4490 || LOSS : 0.14421631395816803\n",
            "EPISODE :4500 || LOSS : 0.1400476098060608\n",
            "Testing\n",
            "Test Accuracy: 0.3940000000000001\n",
            "EPISODE :4510 || LOSS : 0.12103813886642456\n",
            "EPISODE :4520 || LOSS : 0.11184534430503845\n",
            "EPISODE :4530 || LOSS : 0.145681232213974\n",
            "EPISODE :4540 || LOSS : 0.1674487590789795\n",
            "EPISODE :4550 || LOSS : 0.1577092409133911\n",
            "Testing\n",
            "Test Accuracy: 0.37366666666666665\n",
            "EPISODE :4560 || LOSS : 0.15616700053215027\n",
            "EPISODE :4570 || LOSS : 0.16111436486244202\n",
            "EPISODE :4580 || LOSS : 0.14036870002746582\n",
            "EPISODE :4590 || LOSS : 0.12366487830877304\n",
            "EPISODE :4600 || LOSS : 0.13535688817501068\n",
            "Testing\n",
            "Test Accuracy: 0.39\n",
            "EPISODE :4610 || LOSS : 0.13899609446525574\n",
            "EPISODE :4620 || LOSS : 0.1355753391981125\n",
            "EPISODE :4630 || LOSS : 0.12941762804985046\n",
            "EPISODE :4640 || LOSS : 0.13698585331439972\n",
            "EPISODE :4650 || LOSS : 0.11461430042982101\n",
            "Testing\n",
            "Test Accuracy: 0.385\n",
            "EPISODE :4660 || LOSS : 0.14059722423553467\n",
            "EPISODE :4670 || LOSS : 0.13208012282848358\n",
            "EPISODE :4680 || LOSS : 0.1295519322156906\n",
            "EPISODE :4690 || LOSS : 0.1340956836938858\n",
            "EPISODE :4700 || LOSS : 0.11730888485908508\n",
            "Testing\n",
            "Test Accuracy: 0.371\n",
            "EPISODE :4710 || LOSS : 0.14241909980773926\n",
            "EPISODE :4720 || LOSS : 0.1457977592945099\n",
            "EPISODE :4730 || LOSS : 0.1378191113471985\n",
            "EPISODE :4740 || LOSS : 0.16262437403202057\n",
            "EPISODE :4750 || LOSS : 0.1298290640115738\n",
            "Testing\n",
            "Test Accuracy: 0.3846666666666667\n",
            "EPISODE :4760 || LOSS : 0.13342905044555664\n",
            "EPISODE :4770 || LOSS : 0.1233799010515213\n",
            "EPISODE :4780 || LOSS : 0.14983347058296204\n",
            "EPISODE :4790 || LOSS : 0.12425721436738968\n",
            "EPISODE :4800 || LOSS : 0.1569032073020935\n",
            "Testing\n",
            "Test Accuracy: 0.381\n",
            "EPISODE :4810 || LOSS : 0.14634814858436584\n",
            "EPISODE :4820 || LOSS : 0.13220971822738647\n",
            "EPISODE :4830 || LOSS : 0.08870157599449158\n",
            "EPISODE :4840 || LOSS : 0.15157119929790497\n",
            "EPISODE :4850 || LOSS : 0.12515124678611755\n",
            "Testing\n",
            "Test Accuracy: 0.3853333333333333\n",
            "EPISODE :4860 || LOSS : 0.13454806804656982\n",
            "EPISODE :4870 || LOSS : 0.14530128240585327\n",
            "EPISODE :4880 || LOSS : 0.12272942066192627\n",
            "EPISODE :4890 || LOSS : 0.1404951512813568\n",
            "EPISODE :4900 || LOSS : 0.12321028858423233\n",
            "Testing\n",
            "Test Accuracy: 0.3886666666666666\n",
            "EPISODE :4910 || LOSS : 0.11706385016441345\n",
            "EPISODE :4920 || LOSS : 0.13395695388317108\n",
            "EPISODE :4930 || LOSS : 0.11413566023111343\n",
            "EPISODE :4940 || LOSS : 0.16490088403224945\n",
            "EPISODE :4950 || LOSS : 0.13305167853832245\n",
            "Testing\n",
            "Test Accuracy: 0.36200000000000004\n",
            "EPISODE :4960 || LOSS : 0.13094718754291534\n",
            "EPISODE :4970 || LOSS : 0.10974043607711792\n",
            "EPISODE :4980 || LOSS : 0.10701407492160797\n",
            "EPISODE :4990 || LOSS : 0.10536757111549377\n",
            "EPISODE :5000 || LOSS : 0.15769442915916443\n",
            "Testing\n",
            "Test Accuracy: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzSz0_R1mLXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}